{"cells":[{"cell_type":"markdown","metadata":{"id":"_1ejPNgUZff2"},"source":["This code is only for : **Hate Speech Detection in Hinglish Language**\n","\n","Major Highlight of Mark 12 : **Mark 11 Combinational Code has been updated and 3 most important changes have been made :-**\n","\n","1.   New feature of converting Video to Audio and then passign that newly created Audio file in the system for classification has been added.\n","2.   Emoticons Angle Version 1 has been added successfully. An array of hateful Emoticons has been added in the code itself. If the user input has any of emoticon present in this array then the entered text will be marked as Hate.\n","3.   Dataset has been updated. New Dataset contains data from HASOC 2023 + Github Dataset(1) + Github Dataset(2) + Kaggle Dataset\n","\n","\n","Major Highlight of Mark 13 will be : **NOT YET DECIDED**\n","\n","Dataset : **hate_speech (4).tsv**"]},{"cell_type":"markdown","source":["# **Installing and Importing Libraries**"],"metadata":{"id":"D9OKI9uuMoAC"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"USbU3LUrZQ7a","outputId":"e9aec4a7-7df2-459e-b7e5-d7295060ba58","executionInfo":{"status":"ok","timestamp":1710475515387,"user_tz":-330,"elapsed":74966,"user":{"displayName":"Tanmay Nigade","userId":"01004291536742582348"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n","Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.25.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n","Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.25.2)\n","Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.3.0)\n","Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.0)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n","Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.7)\n","Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.6)\n","Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n","Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.25.2)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.0)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.10.0)\n","Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.36.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.62.1)\n","Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.2)\n","Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n","Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.27.0)\n","Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.5.2)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.1)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.3)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.3.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.4.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2024.2.2)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.5)\n","Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.5.1)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.38.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.2)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.10.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n","Requirement already satisfied: google-cloud-speech in /usr/local/lib/python3.10/dist-packages (2.25.1)\n","Requirement already satisfied: google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1 in /usr/local/lib/python3.10/dist-packages (from google-cloud-speech) (2.11.1)\n","Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from google-cloud-speech) (2.27.0)\n","Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-speech) (1.23.0)\n","Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 in /usr/local/lib/python3.10/dist-packages (from google-cloud-speech) (3.20.3)\n","Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-speech) (1.63.0)\n","Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-speech) (2.31.0)\n","Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-speech) (1.62.1)\n","Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-speech) (1.48.2)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-speech) (5.3.3)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-speech) (0.3.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-speech) (4.9)\n","Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-speech) (0.5.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-speech) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-speech) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-speech) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-speech) (2024.2.2)\n","Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","python3.10-venv is already the newest version (3.10.12-1~22.04.3).\n","0 upgraded, 0 newly installed, 0 to remove and 38 not upgraded.\n","Requirement already satisfied: google-cloud-videointelligence in /usr/local/lib/python3.10/dist-packages (2.13.3)\n","Requirement already satisfied: google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1 in /usr/local/lib/python3.10/dist-packages (from google-cloud-videointelligence) (2.11.1)\n","Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from google-cloud-videointelligence) (2.27.0)\n","Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-videointelligence) (1.23.0)\n","Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 in /usr/local/lib/python3.10/dist-packages (from google-cloud-videointelligence) (3.20.3)\n","Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-videointelligence) (1.63.0)\n","Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-videointelligence) (2.31.0)\n","Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-videointelligence) (1.62.1)\n","Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-videointelligence) (1.48.2)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-videointelligence) (5.3.3)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-videointelligence) (0.3.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-videointelligence) (4.9)\n","Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-videointelligence) (0.5.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-videointelligence) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-videointelligence) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-videointelligence) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-videointelligence) (2024.2.2)\n","Requirement already satisfied: pytchat in /usr/local/lib/python3.10/dist-packages (0.5.5)\n","Requirement already satisfied: httpx[http2] in /usr/local/lib/python3.10/dist-packages (from pytchat) (0.27.0)\n","Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx[http2]->pytchat) (3.7.1)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx[http2]->pytchat) (2024.2.2)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx[http2]->pytchat) (1.0.4)\n","Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx[http2]->pytchat) (3.6)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx[http2]->pytchat) (1.3.1)\n","Requirement already satisfied: h2<5,>=3 in /usr/local/lib/python3.10/dist-packages (from httpx[http2]->pytchat) (4.1.0)\n","Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx[http2]->pytchat) (0.14.0)\n","Requirement already satisfied: hyperframe<7,>=6.0 in /usr/local/lib/python3.10/dist-packages (from h2<5,>=3->httpx[http2]->pytchat) (6.0.1)\n","Requirement already satisfied: hpack<5,>=4.0 in /usr/local/lib/python3.10/dist-packages (from h2<5,>=3->httpx[http2]->pytchat) (4.0.0)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx[http2]->pytchat) (1.2.0)\n","Requirement already satisfied: google-cloud-vision in /usr/local/lib/python3.10/dist-packages (3.7.2)\n","Requirement already satisfied: google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1 in /usr/local/lib/python3.10/dist-packages (from google-cloud-vision) (2.11.1)\n","Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from google-cloud-vision) (2.27.0)\n","Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-vision) (1.23.0)\n","Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 in /usr/local/lib/python3.10/dist-packages (from google-cloud-vision) (3.20.3)\n","Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-vision) (1.63.0)\n","Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-vision) (2.31.0)\n","Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-vision) (1.62.1)\n","Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-vision) (1.48.2)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-vision) (5.3.3)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-vision) (0.3.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-vision) (4.9)\n","Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0dev,>=2.14.1->google-cloud-vision) (0.5.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-vision) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-vision) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-vision) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-vision) (2024.2.2)\n","Requirement already satisfied: moviepy in /usr/local/lib/python3.10/dist-packages (1.0.3)\n","Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.10/dist-packages (from moviepy) (4.4.2)\n","Requirement already satisfied: tqdm<5.0,>=4.11.2 in /usr/local/lib/python3.10/dist-packages (from moviepy) (4.66.2)\n","Requirement already satisfied: requests<3.0,>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from moviepy) (2.31.0)\n","Requirement already satisfied: proglog<=1.0.0 in /usr/local/lib/python3.10/dist-packages (from moviepy) (0.1.10)\n","Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from moviepy) (1.25.2)\n","Requirement already satisfied: imageio<3.0,>=2.5 in /usr/local/lib/python3.10/dist-packages (from moviepy) (2.31.6)\n","Requirement already satisfied: imageio-ffmpeg>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from moviepy) (0.4.9)\n","Requirement already satisfied: pillow<10.1.0,>=8.3.2 in /usr/local/lib/python3.10/dist-packages (from imageio<3.0,>=2.5->moviepy) (9.4.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from imageio-ffmpeg>=0.2.0->moviepy) (67.7.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (2024.2.2)\n","Requirement already satisfied: ffmpeg in /usr/local/lib/python3.10/dist-packages (1.4)\n"]}],"source":["!pip install pandas\n","!pip install numpy\n","!pip install scikit-learn\n","!pip install tensorflow\n","!pip install transformers\n","!pip install google-cloud-speech\n","!apt install python3.10-venv\n","!python -m venv speech_to_text_demo\n","!source speech_to_text_demo/bin/activate\n","!pip install google-cloud-videointelligence\n","!pip install pytchat\n","!pip install google-cloud-vision\n","!pip install moviepy\n","!pip install --upgrade ffmpeg"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ob1UjBbX0HH6","outputId":"fc121072-6b93-46bd-f890-97d114b52e12","executionInfo":{"status":"ok","timestamp":1710475614735,"user_tz":-330,"elapsed":25211,"user":{"displayName":"Tanmay Nigade","userId":"01004291536742582348"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"yN3CKPBg0J7I","executionInfo":{"status":"ok","timestamp":1710475629189,"user_tz":-330,"elapsed":11095,"user":{"displayName":"Tanmay Nigade","userId":"01004291536742582348"}}},"outputs":[],"source":["# Import necessary libraries\n","import os\n","import io\n","import pandas as pd\n","import numpy as np\n","import tensorflow as tf\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","from transformers import BertTokenizer, TFBertForSequenceClassification\n","from transformers import BertConfig\n","from transformers import AdamWeightDecay\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.losses import SparseCategoricalCrossentropy\n","from tensorflow.keras.metrics import SparseCategoricalAccuracy\n","from tensorflow.keras.layers import Dropout\n","from sklearn.metrics import classification_report\n","from google.cloud import speech_v1\n","from IPython.display import display, Audio\n","from google.cloud import vision_v1\n","from google.cloud import videointelligence_v1p3beta1 as videointelligence\n","from google.cloud.videointelligence_v1p3beta1.services.video_intelligence_service import VideoIntelligenceServiceClient\n","from google.cloud.videointelligence_v1p3beta1.types import Feature, VideoContext, TextDetectionConfig\n","from google.cloud.videointelligence_v1 import AnnotateVideoRequest, VideoContext, Feature, TextDetectionConfig\n","import moviepy.editor as mp\n","from google.colab import files\n","import pytchat\n","import time\n","import torch"]},{"cell_type":"markdown","source":["# **Training the Hate Speech Detection Model**"],"metadata":{"id":"7sqG63SKMvCq"}},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["6ce40a102bb349958df56f815697a17f","652d793d5c294f0fa7473850e1893f6b","cf74e34b7d48410fb554066a44ead22f","1c5c56fd03e446d09cd380e329dcfd44","8c9096046534404e8b0f78adc1f7cc57","bc990974955b48879f148f72aad63802","7397f954506249edb9caa9d5e2025b63","4d088bd94cbc450d9d0efd548b25db2d","9e6be062e8ca47cda7af67a11aa92f3b","74e14952bb4949ab916561bde189600e","8b04abcce86949b684061627cf47ac93","bdc56fe63878491bb075fb65a0de8aa6","c07f78d4ee1e416ca71291b1c94a4a89","bef766e410c04507ac6427db397bdbce","4bc04a3f630b4061ac32ddcb2db9e567","243032496ddc40f9aaccf97bfd7d874f","fab1ac409f9a4175bbc5da65c3e1e1ce","2bab0914e1b34841bce78f200fdef377","e16cfd431df0464bbfb25c2ce7647e77","827382cacd204da49f7cdd6f4eef31cd","4708dc2c5b754326855485c040913963","abe00225682d4f7680a06a2f25e468e4","4a45c64ddbe14af6ae62bc9b830fbbc6","ccd81dc7a9ef445bbac776baad876836","dac9a0c8efac40e4ade2ffc8e9644c53","4c123d3be27f4f95bfe17d1cf4de4f6c","46c2cadd9b324eee8f1688eed416bf54","779765c8af3144fabe0b376159cf744a","9c46bdc272cd4d9da40bbdd827575f9a","bc9796f07ed045109208d36c9ae9eaf5","3b36f7e89c6443fab2a71eccf7ec4e15","1d31317f901b4bffbf39a138dabae8bc","9ee886a69e544669ac512b1798ed18fb","949be1ef70744078807f0d4e7528b5a8","e3396e3346954e55b1ea363da3a32927","aa6955c1807441df948d323d6a645240","aea5f874c1d3400b96dd4372ec05096f","30bb877dd92543c89eca3a995bc81caf","abf5bb3491e5492592040c64b88c9f05","6821d48ba2c34c859831c989ad4748b5","1a0a1887b99e4c03b9f219afca28a2a2","98726540e6894a2d952bc52d3455ccea","e09c73b386a34687af1176b295f6ba8c","01852540562341788750bfe2f25d374a","2da1804921da4dfe878b825a764ad488","6d71b335a0f646098fcaf4472c470ceb","6fb15e1dad8c457c8ed702f6333c7c3f","87f3059573914594ab7d2432acc6edda","dcc312e075b849b9acd33035fe574b18","e6bee8f8b9534e9cabce0412bc5b2696","138a2008312643e8a5f21ffdc50616a1","469e4dcb49194a41bc628c050406e470","11d03462c5e645b3a80cae6bbe5c0303","04ea76eb366e4ba381936713e1a5b63c","d89b3024aa41400d82a7f49cb65a4c53"]},"id":"1ZU3Fq470MH-","outputId":"d7dc4059-4cca-45c1-eb8e-617dc5c73520","executionInfo":{"status":"ok","timestamp":1710481411707,"user_tz":-330,"elapsed":5715640,"user":{"displayName":"Tanmay Nigade","userId":"01004291536742582348"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n","\n"]},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6ce40a102bb349958df56f815697a17f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.txt:   0%|          | 0.00/872k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bdc56fe63878491bb075fb65a0de8aa6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/1.72M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a45c64ddbe14af6ae62bc9b830fbbc6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"949be1ef70744078807f0d4e7528b5a8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/672M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2da1804921da4dfe878b825a764ad488"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n","\n","Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/16\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:AutoGraph could not transform <function infer_framework at 0x7e81fbeef370> and will run it as-is.\n","Cause: for/else statement not yet supported\n","To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"]},{"output_type":"stream","name":"stdout","text":["WARNING: AutoGraph could not transform <function infer_framework at 0x7e81fbeef370> and will run it as-is.\n","Cause: for/else statement not yet supported\n","To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n","200/200 [==============================] - 428s 2s/step - loss: 0.5648 - accuracy: 0.6633 - val_loss: 0.4911 - val_accuracy: 0.7125\n","Epoch 2/16\n","200/200 [==============================] - 335s 2s/step - loss: 0.4393 - accuracy: 0.7288 - val_loss: 0.4586 - val_accuracy: 0.7417\n","Epoch 3/16\n","200/200 [==============================] - 334s 2s/step - loss: 0.3700 - accuracy: 0.7547 - val_loss: 0.5158 - val_accuracy: 0.7634\n","Epoch 4/16\n","200/200 [==============================] - 349s 2s/step - loss: 0.3144 - accuracy: 0.7729 - val_loss: 0.5495 - val_accuracy: 0.7804\n","Epoch 5/16\n","200/200 [==============================] - 349s 2s/step - loss: 0.2662 - accuracy: 0.7879 - val_loss: 0.6031 - val_accuracy: 0.7936\n","Epoch 6/16\n","200/200 [==============================] - 349s 2s/step - loss: 0.2295 - accuracy: 0.7999 - val_loss: 0.6740 - val_accuracy: 0.8047\n","Epoch 7/16\n","200/200 [==============================] - 335s 2s/step - loss: 0.2050 - accuracy: 0.8097 - val_loss: 0.7088 - val_accuracy: 0.8137\n","Epoch 8/16\n","200/200 [==============================] - 334s 2s/step - loss: 0.1862 - accuracy: 0.8177 - val_loss: 0.9843 - val_accuracy: 0.8211\n","Epoch 9/16\n","200/200 [==============================] - 349s 2s/step - loss: 0.1724 - accuracy: 0.8245 - val_loss: 0.9373 - val_accuracy: 0.8271\n","Epoch 10/16\n","200/200 [==============================] - 349s 2s/step - loss: 0.1560 - accuracy: 0.8302 - val_loss: 1.0222 - val_accuracy: 0.8327\n","Epoch 11/16\n","200/200 [==============================] - 349s 2s/step - loss: 0.1503 - accuracy: 0.8351 - val_loss: 1.1253 - val_accuracy: 0.8372\n","Epoch 12/16\n","200/200 [==============================] - 335s 2s/step - loss: 0.1447 - accuracy: 0.8393 - val_loss: 0.9084 - val_accuracy: 0.8410\n","Epoch 13/16\n","200/200 [==============================] - 349s 2s/step - loss: 0.1330 - accuracy: 0.8429 - val_loss: 1.0261 - val_accuracy: 0.8445\n","Epoch 14/16\n","200/200 [==============================] - 349s 2s/step - loss: 0.1273 - accuracy: 0.8462 - val_loss: 1.0969 - val_accuracy: 0.8476\n","Epoch 15/16\n","200/200 [==============================] - 335s 2s/step - loss: 0.1278 - accuracy: 0.8491 - val_loss: 1.1968 - val_accuracy: 0.8504\n","Epoch 16/16\n","200/200 [==============================] - 349s 2s/step - loss: 0.1224 - accuracy: 0.8516 - val_loss: 1.4491 - val_accuracy: 0.8527\n","215/215 [==============================] - 60s 277ms/step - loss: 1.4543 - accuracy: 0.8503\n","Test Loss: 1.4543, Test Accuracy: 0.8503\n"]}],"source":["# Load your dataset from the local file (Scenario 1)\n","dataset_path = '/content/drive/MyDrive/Hate Speech Hinglish Laguage/hate_speech (3) (1).tsv'\n","comment = pd.read_csv(dataset_path, sep='\\t', names=[\"comment\", \"label\"])\n","\n","# Specify the text and label column names\n","text_column_name = 'comment'\n","label_column_name = 'label'\n","\n","# Split the dataset into training and testing sets\n","train_df, test_df = train_test_split(comment, test_size=0.3, random_state=42)\n","\n","# Initialize the label encoder\n","label_encoder = LabelEncoder()\n","label_encoder.fit(train_df[label_column_name])\n","\n","# Encode labels in both train and test datasets\n","train_df['label_encoded'] = label_encoder.transform(train_df[label_column_name])\n","test_df['label_encoded'] = label_encoder.transform(test_df[label_column_name])\n","\n","# Load the pre-trained BERT tokenizer (Scenario 1)\n","tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-uncased', do_lower_case=True)\n","\n","# Tokenize and preprocess the text data (Scenario 1)\n","max_length = 128\n","train_input_ids = tokenizer(list(train_df['comment'].astype(str)), padding=True, truncation=True, max_length=max_length, return_tensors='tf')\n","test_input_ids = tokenizer(list(test_df['comment'].astype(str)), padding=True, truncation=True, max_length=max_length, return_tensors='tf')\n","\n","# Load the pre-trained BERT model for sequence classification (Scenario 1)\n","config = BertConfig.from_pretrained('bert-base-multilingual-uncased', num_labels=len(label_encoder.classes_))\n","bert_model = TFBertForSequenceClassification.from_pretrained('bert-base-multilingual-uncased', config=config)\n","\n","# Add Dropout layer (Scenario 1)\n","dropout_rate = 0.1\n","bert_model.layers[0].embeddings.dropout = Dropout(dropout_rate)\n","\n","# Set up the optimizer and loss function (Scenario 1)\n","optimizer = AdamWeightDecay(learning_rate=3e-5, weight_decay_rate=0.01)\n","loss_function = SparseCategoricalCrossentropy(from_logits=True)\n","metric = SparseCategoricalAccuracy('accuracy')\n","\n","# Compile the model using the Adam optimizer (Scenario 1)\n","bert_model.compile(optimizer=optimizer, loss=loss_function, metrics=[metric])\n","\n","# Train the model (Scenario 1)\n","epochs = 16\n","batch_size = 64\n","history = bert_model.fit(\n","    train_input_ids,\n","    train_df['label_encoded'],\n","    epochs=epochs,\n","    batch_size=batch_size,\n","    validation_split=0.2\n",")\n","\n","# Evaluate the model (Scenario 1)\n","test_loss, test_accuracy = bert_model.evaluate(test_input_ids, test_df['label_encoded'])\n","print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}')"]},{"cell_type":"code","source":["# Make predictions on the test set\n","predictions = np.argmax(bert_model.predict(test_input_ids).logits, axis=1)\n","\n","# Print classification report\n","print(classification_report(test_df['label_encoded'], predictions))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Fw0eoeywo-cy","executionInfo":{"status":"ok","timestamp":1710481610237,"user_tz":-330,"elapsed":88392,"user":{"displayName":"Tanmay Nigade","userId":"01004291536742582348"}},"outputId":"77284d6b-268b-40dd-b581-cf2685455d38"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["215/215 [==============================] - 88s 278ms/step\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","\n","WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","\n","WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       0.00      0.00      0.00         1\n","           1       0.77      0.83      0.79      4420\n","           2       0.00      0.00      0.00         1\n","           3       0.63      0.54      0.58      2428\n","           4       1.00      0.60      0.75         5\n","\n","    accuracy                           0.72      6855\n","   macro avg       0.48      0.39      0.43      6855\n","weighted avg       0.72      0.72      0.72      6855\n","\n"]}]},{"cell_type":"code","source":["from sklearn.metrics import accuracy_score\n","# ...\n","\n","# Evaluate the model\n","test_loss, test_accuracy = bert_model.evaluate(test_input_ids, test_df['label_encoded'])\n","print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RWbifldFpP2g","executionInfo":{"status":"ok","timestamp":1710481710398,"user_tz":-330,"elapsed":82463,"user":{"displayName":"Tanmay Nigade","userId":"01004291536742582348"}},"outputId":"a4827a7b-c625-49fe-8866-556752777794"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["215/215 [==============================] - 59s 276ms/step - loss: 1.4543 - accuracy: 0.8471\n","Test Loss: 1.4543, Test Accuracy: 0.8471\n"]}]},{"cell_type":"code","source":["def predict_hate_speech(text, model, tokenizer, label_encoder):\n","    # Tokenize and preprocess the input text\n","    encoding = tokenizer.encode_plus(\n","        text,\n","        add_special_tokens=True,\n","        max_length=max_length,\n","        padding='max_length',\n","        truncation=True,\n","        return_tensors='tf'\n","    )\n","\n","    input_ids = encoding['input_ids']\n","    attention_mask = encoding['attention_mask']\n","\n","    # Make prediction\n","    with tf.device('/cpu:0'):  # Ensure predictions are made on CPU\n","        outputs = model.predict([input_ids, attention_mask])\n","        logits = outputs.logits\n","\n","    # Convert logits to probabilities and get the predicted label\n","    probabilities = tf.nn.softmax(logits, axis=1).numpy()[0]\n","    predicted_label_id = np.argmax(probabilities)\n","    predicted_label = label_encoder.classes_[predicted_label_id]\n","\n","    return predicted_label\n","\n","# ... (rest of the code remains unchanged)\n","\n","# Take user input\n","user_input = input(\"Enter a text to classify: \")\n","\n","# Make prediction\n","predicted_label = predict_hate_speech(user_input, bert_model, tokenizer, label_encoder)\n","\n","print(f'Text: \"{user_input}\"')\n","print(f'Predicted Label: {predicted_label}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RV5yfeIcpRdG","executionInfo":{"status":"ok","timestamp":1710481736763,"user_tz":-330,"elapsed":19854,"user":{"displayName":"Tanmay Nigade","userId":"01004291536742582348"}},"outputId":"508c50d1-ab4d-4808-fe8a-d3a466264703"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Enter a text to classify: harami insan\n","1/1 [==============================] - 9s 9s/step\n","Text: \"harami insan\"\n","Predicted Label: yes\n"]}]},{"cell_type":"code","source":["!pip install joblib\n","import joblib\n","\n","from transformers import TFBertForSequenceClassification, BertTokenizer\n","\n","# Save the BERT model and tokenizer\n","model_directory = '/content/drive/MyDrive/Hate Speech Hinglish Laguage/hate_speech_model'\n","bert_model.save_pretrained(model_directory)\n","tokenizer.save_pretrained(model_directory)\n","\n","# Save the TensorFlow model\n","tf_model_filename = '/content/drive/MyDrive/Hate Speech Hinglish Laguage/tf_model.h5'\n","bert_model.save_weights(tf_model_filename)\n","\n","# Save the label encoder\n","label_encoder_filename = '/content/drive/MyDrive/Hate Speech Hinglish Laguage/label_encoder.pkl'\n","joblib.dump(label_encoder, label_encoder_filename)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Xu038m9GqCwi","executionInfo":{"status":"ok","timestamp":1710481960605,"user_tz":-330,"elapsed":31123,"user":{"displayName":"Tanmay Nigade","userId":"01004291536742582348"}},"outputId":"6a4c788e-5336-4440-b782-26932c7c5127"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (1.3.2)\n"]},{"output_type":"execute_result","data":{"text/plain":["['/content/drive/MyDrive/Hate Speech Hinglish Laguage/label_encoder.pkl']"]},"metadata":{},"execution_count":9}]},{"cell_type":"markdown","source":["*Saved Model Code Directily Run below cell *"],"metadata":{"id":"TVuAM_T2sCQk"}},{"cell_type":"code","source":["from transformers import TFBertForSequenceClassification, BertTokenizer\n","\n","# Load the BERT model and tokenizer\n","model_directory = '/content/drive/MyDrive/Hate Speech Hinglish Laguage/hate_speech_model'\n","loaded_model = TFBertForSequenceClassification.from_pretrained(model_directory)\n","loaded_tokenizer = BertTokenizer.from_pretrained(model_directory)\n","\n","# Load the TensorFlow model\n","tf_model_filename = '/content/drive/MyDrive/Hate Speech Hinglish Laguage/tf_model.h5'\n","loaded_model.load_weights(tf_model_filename)\n","\n","# Load the label encoder\n","label_encoder_filename = '/content/drive/MyDrive/Hate Speech Hinglish Laguage/label_encoder.pkl'\n","loaded_label_encoder = joblib.load(label_encoder_filename)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"z1mJDA-6q4Or","executionInfo":{"status":"ok","timestamp":1710482035618,"user_tz":-330,"elapsed":11076,"user":{"displayName":"Tanmay Nigade","userId":"01004291536742582348"}},"outputId":"faebf57c-4db7-431a-9f83-e6c0d3845071"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stderr","text":["Some layers from the model checkpoint at /content/drive/MyDrive/Hate Speech Hinglish Laguage/hate_speech_model were not used when initializing TFBertForSequenceClassification: ['dropout_37']\n","- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at /content/drive/MyDrive/Hate Speech Hinglish Laguage/hate_speech_model.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n"]}]},{"cell_type":"code","source":["import joblib\n","from transformers import TFBertForSequenceClassification, BertTokenizer\n","import tensorflow as tf\n","import numpy as np\n","\n","def load_model_and_predict(text):\n","    # Load the BERT model and tokenizer\n","    model_directory = '/content/drive/MyDrive/Hate Speech Hinglish Laguage/hate_speech_model'\n","    loaded_model = TFBertForSequenceClassification.from_pretrained(model_directory)\n","    loaded_tokenizer = BertTokenizer.from_pretrained(model_directory)\n","\n","    # Load the TensorFlow model\n","    tf_model_filename = '/content/drive/MyDrive/Hate Speech Hinglish Laguage/tf_model.h5'\n","    loaded_model.load_weights(tf_model_filename)\n","\n","    # Load the label encoder\n","    label_encoder_filename = '/content/drive/MyDrive/Hate Speech Hinglish Laguage/label_encoder.pkl'\n","    loaded_label_encoder = joblib.load(label_encoder_filename)\n","\n","    # Tokenize and preprocess the input text\n","    encoding = loaded_tokenizer.encode_plus(\n","        text,\n","        add_special_tokens=True,\n","        max_length=128,\n","        padding='max_length',\n","        truncation=True,\n","        return_tensors='tf'\n","    )\n","\n","    input_ids = encoding['input_ids']\n","    attention_mask = encoding['attention_mask']\n","\n","    # Make prediction\n","    with tf.device('/cpu:0'):  # Ensure predictions are made on CPU\n","        outputs = loaded_model.predict([input_ids, attention_mask])\n","        logits = outputs.logits\n","\n","    # Convert logits to probabilities and get the predicted label\n","    probabilities = tf.nn.softmax(logits, axis=1).numpy()[0]\n","    predicted_label_id = np.argmax(probabilities)\n","    predicted_label = loaded_label_encoder.classes_[predicted_label_id]\n","\n","    return predicted_label\n","\n","# Example usage:\n","user_input = input(\"Enter a text to classify: \")\n","predicted_label = load_model_and_predict(user_input)\n","print(f'Predicted Label: {predicted_label}')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XLAFcXoYrXRf","executionInfo":{"status":"ok","timestamp":1710482170147,"user_tz":-330,"elapsed":22845,"user":{"displayName":"Tanmay Nigade","userId":"01004291536742582348"}},"outputId":"151250bd-688d-430f-e01d-cb89c4da8ebf"},"execution_count":13,"outputs":[{"name":"stdout","output_type":"stream","text":["Enter a text to classify: harami insan\n"]},{"output_type":"stream","name":"stderr","text":["Some layers from the model checkpoint at /content/drive/MyDrive/Hate Speech Hinglish Laguage/hate_speech_model were not used when initializing TFBertForSequenceClassification: ['dropout_37']\n","- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at /content/drive/MyDrive/Hate Speech Hinglish Laguage/hate_speech_model.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n"]},{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 11s 11s/step\n","Predicted Label: yes\n"]}]},{"cell_type":"markdown","source":["# **Defining User Scenarios**"],"metadata":{"id":"ZZHkpH7DM_eg"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Rc9oOsyP0ObJ"},"outputs":[],"source":["import re\n","\n","hateful_emojis = hateful_emojis = [u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'', u'']\n","\n","def has_hateful_emoji(text):\n","    for emoji in hateful_emojis:\n","        if emoji in text:\n","            return True\n","    return False\n","\n","def predict_hate_speech(text, model, tokenizer, label_encoder, device):\n","    # Check for hateful emojis\n","    if has_hateful_emoji(text):\n","        return \"yes\"  # If hateful emoji is detected, classify as Hate\n","\n","    encoding = tokenizer.encode_plus(\n","        text,\n","        add_special_tokens=True,\n","        max_length=max_length,\n","        padding='max_length',\n","        truncation=True,\n","        return_tensors='tf'  # Use TensorFlow tensors consistently\n","    )\n","\n","    input_ids = encoding['input_ids']\n","    attention_mask = encoding['attention_mask']\n","\n","    with tf.device('GPU:0'):  # Change 'GPU:0' to match your GPU device name\n","        outputs = model.predict([input_ids, attention_mask])\n","        logits = outputs.logits\n","\n","    probabilities = tf.nn.softmax(logits, axis=1)\n","    predicted_label_id = tf.argmax(probabilities, axis=1).numpy()[0]\n","    predicted_label = label_encoder.classes_[predicted_label_id]\n","\n","    return predicted_label\n","\n","# Function for Hate Speech Detection on Audio (Scenario 2)\n","# Function for Hate Speech Detection on Audio\n","def predict_hate_speech_audio(audio_content, model, tokenizer, label_encoder, device):\n","    # Convert audio to text using Google Speech-to-Text\n","    text = convert_audio_to_text(audio_content)\n","\n","    # Use the Hate Speech Detection model for prediction\n","    predicted_label = predict_hate_speech(text, model, tokenizer, label_encoder, device)\n","\n","    return predicted_label\n","\n","# Function for Hate Speech Detection on Image (Scenario 3)\n","def predict_hate_speech_image(image_content, model, tokenizer, label_encoder, device):\n","    # Extract text from image using Google Cloud Vision API\n","    detected_text = detect_text_from_image(image_content)\n","\n","    # Use the Hate Speech Detection model for prediction\n","    predicted_label = predict_hate_speech(detected_text, model, tokenizer, label_encoder, device)\n","\n","    return predicted_label\n","\n","# Function for Hate Speech Detection on Live YouTube Video (Scenario 5)\n","def predict_hate_speech_live_youtube(video_id, model, tokenizer, label_encoder, device):\n","    # Get live comments from a live YouTube video\n","    live_comments_df = get_live_comments(video_id)\n","\n","    # Apply Hate Speech Detection on live comments\n","    classified_live_comments = []\n","\n","    for live_comment_text in live_comments_df['message']:\n","        predicted_label = predict_hate_speech(live_comment_text, model, tokenizer, label_encoder, device)\n","        classified_live_comments.append((live_comment_text, predicted_label))\n","\n","    return classified_live_comments\n","\n","# Function to convert audio to text using Google Speech-to-Text (Scenario 2)\n","# Function to convert audio to text using Google Speech-to-Text\n","def convert_audio_to_text(audio_content):\n","    client = speech_v1.SpeechClient()\n","\n","    # Configure audio recognition settings\n","    config = speech_v1.RecognitionConfig(\n","        encoding=speech_v1.RecognitionConfig.AudioEncoding.MP3,\n","        sample_rate_hertz=44100,\n","        language_code=\"en-US\",\n","        model=\"video\",\n","    )\n","\n","    audio = speech_v1.RecognitionAudio(content=audio_content)\n","\n","    # Recognize speech\n","    response = client.recognize(config=config, audio=audio)\n","\n","    # Extract the transcriptions\n","    transcriptions = [result.alternatives[0].transcript for result in response.results]\n","\n","    # Concatenate transcriptions into a single text\n","    text = ' '.join(transcriptions)\n","\n","    return text\n","\n","# Set Google Cloud Speech-to-Text API credentials\n","os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = r\"/content/drive/MyDrive/Hate Speech Detection Project/Project Execution/Backend Development/Hate Speech Detection Models/Mark 6/Speech Recognition/SR_Mark 1_Google STT/noted-casing-413617-1b32975e81c6.json\"\n","\n","# Function to detect text from an image using Google Cloud Vision API (Scenario 3)\n","def detect_text_from_image(image_content):\n","    os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = r\"/content/drive/MyDrive/Hate Speech Detection Project/Project Execution/Backend Development/Hate Speech Detection Models/Mark 7/Text Detection/noted-casing-413617-3e02ca546b83.json\"\n","\n","    vision_client = vision_v1.ImageAnnotatorClient()\n","\n","    image = vision_v1.Image(content=image_content)\n","\n","    response = vision_client.text_detection(image=image)\n","\n","    texts = response.text_annotations\n","    detected_text = \"\"\n","    for text in texts:\n","        detected_text += text.description + \" \"\n","\n","    return detected_text.strip()\n","\n","\n","# Function for Live YouTube Video Comments (Scenario 5)\n","def get_live_comments_with_timeout(video_id, timeout_seconds=60):\n","    chat = pytchat.create(video_id=video_id)\n","\n","    live_comments = []\n","    start_time = time.time()\n","\n","    while chat.is_alive():\n","        for c in chat.get().sync_items():\n","            live_comments.append([c.datetime, c.author.name, c.message])\n","\n","        # Check timeout\n","        elapsed_time = time.time() - start_time\n","        if elapsed_time >= timeout_seconds:\n","            break\n","\n","    return pd.DataFrame(live_comments, columns=['datetime', 'author', 'message'])\n","\n","def classify_live_comments(comments_df, model, tokenizer, label_encoder, device):\n","    classified_live_comments = []\n","\n","    for live_comment_text in comments_df['message']:\n","        predicted_label = predict_hate_speech(live_comment_text, model, tokenizer, label_encoder, device)\n","        classified_live_comments.append((live_comment_text, predicted_label))\n","\n","    return pd.DataFrame(classified_live_comments, columns=['live_comment_text', 'hate_speech_label'])\n","\n","# Device specification (remove any explicit device specification)\n","torch_device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n"]},{"cell_type":"markdown","source":["# **User Interface**"],"metadata":{"id":"wDEzwlPaNJnF"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"uF1a7Xg40RYt","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"34a8103c-e8e9-437b-ec81-dbc07edbdfe4"},"outputs":[{"output_type":"stream","name":"stdout","text":["1 = Text Classification\n","2 = Audio Classification\n","3 = Image Classification\n","4 = GIF Classification\n","5 = Video Classification\n","6 = Youtube Live Classification\n","Enter the scenario choice (1-6), or 'Cancel' to exit: 1\n","Text Classification\n","Enter a text to classify: Gadha ho tum\n","1/1 [==============================] - 0s 53ms/step\n","Text: \"Gadha ho tum\"\n","Predicted Label: no\n","\n","\n","Enter the scenario choice (1-6), or 'Cancel' to exit: 2\n","Audio Classification\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-4a189105-9b09-4851-8dcd-2c80cbab1527\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-4a189105-9b09-4851-8dcd-2c80cbab1527\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving WhatsApp-Audio-2024-02-08-at-05.14.03_b4cbaf2f.waptt.mp3 to WhatsApp-Audio-2024-02-08-at-05.14.03_b4cbaf2f.waptt (1).mp3\n","1/1 [==============================] - 0s 52ms/step\n","Hate Speech Prediction for Audio: no\n","\n","\n","Enter the scenario choice (1-6), or 'Cancel' to exit: 3\n","Image Classification\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-1a61b103-bec5-47ca-828e-76977968d2a7\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-1a61b103-bec5-47ca-828e-76977968d2a7\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving bb062d99dc37d420fa56661330952925.jpg to bb062d99dc37d420fa56661330952925 (1).jpg\n","1/1 [==============================] - 0s 53ms/step\n","Hate Speech Prediction for Image: yes\n","\n","Enter the scenario choice (1-6), or 'Cancel' to exit: 4\n","GIF Classification\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-9f42af47-e321-491b-8000-7b033ebde1b0\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-9f42af47-e321-491b-8000-7b033ebde1b0\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving gandu-tu-gandu.gif to gandu-tu-gandu.gif\n","Started text detection operation. Waiting for results...\n","1/1 [==============================] - 0s 75ms/step\n","Text Extracted from GIF: \"TU GANDU HAI _TU GANDU HAI \"\n","Hate Speech Detection Result: yes\n","\n","\n","Enter the scenario choice (1-6), or 'Cancel' to exit: 5\n","Video Classification\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-1c9553ba-d2bd-4d98-9b59-ecd5dbf2d661\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-1c9553ba-d2bd-4d98-9b59-ecd5dbf2d661\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving Attitude Shayari  New Shayari Status whatsApp Status 10 Second Shayari Mr Shadab.mp4 to Attitude Shayari  New Shayari Status whatsApp Status 10 Second Shayari Mr Shadab.mp4\n","\n","Task 1 : Audio inside Video is processed\n","MoviePy - Writing audio in /content/drive/MyDrive/Hate Speech Detection Project/video_to_udio.mp3\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["MoviePy - Done.\n","1/1 [==============================] - 0s 53ms/step\n","Hate Speech Prediction for Audio: no\n","\n","Task 2 : Text inside Video is processed\n","Started text detection operation. Waiting for results...\n","1/1 [==============================] - 0s 52ms/step\n","Text Extracted from GIF: \"MAUSAM KI O JANE MANS Like NAFRAT Mr S lab KHO GAYI TARAH YUN JANE MANC JO PASAND THI Mr Shadab MAUSAM KIO MAUSAM KI G9 MAUSAM KI e Mr Shaab Mr Shad LIKE, SHARE, SUBSCRIBE JANE MANO JANE MANU APNI BADALTE DEKH  & Mr Shada TUMHE JANE MAN 2 MAUSAM KI JANEMANO LAGTA HAI JANE MAN HO GAYI PASAND SE WO AB KAHI MERI TUMME TARAHYUN \"\n","Hate Speech Detection Result: yes\n","\n","\n","Enter the scenario choice (1-6), or 'Cancel' to exit: 6\n","Youtube Live Classification\n","Enter the YouTube live stream video ID: gCNeDWCI0vo\n","1/1 [==============================] - 0s 74ms/step\n","1/1 [==============================] - 0s 72ms/step\n","1/1 [==============================] - 0s 67ms/step\n","1/1 [==============================] - 0s 68ms/step\n","1/1 [==============================] - 0s 94ms/step\n","1/1 [==============================] - 0s 72ms/step\n","1/1 [==============================] - 0s 70ms/step\n","1/1 [==============================] - 0s 71ms/step\n","                                   live_comment_text hate_speech_label\n","0                             the holy spirit speaks                no\n","1  MY BF IM THE BEST NOT JUST GOOD FOR YOU REMIND...                no\n","2  i am HE/SHE the omnipotent almighty...i am onl...                no\n","3  Putin and Persia. beth togarmah and another ki...                no\n","4  :purple_heart:don't really care living in peac...                no\n","5  DONT GIVE UP SOMEONE TRYING TO STEAL MOTORCYCL...                no\n","6  Sudan, stay strong and decolonized :flexed_bic...                no\n","7  It's not going to happen Daniel ...it's gone h...                no\n","\n","Enter the scenario choice (1-6), or 'Cancel' to exit: Cancel\n","Exiting the program.\n"]}],"source":["print(\"1 = Text Classification\")\n","print(\"2 = Audio Classification\")\n","print(\"3 = Image Classification\")\n","print(\"4 = GIF Classification\")\n","print(\"5 = Video Classification\")\n","print(\"6 = Youtube Live Classification\")\n","\n","while True:\n","    # Take user input for scenario choice\n","    user_choice = input(\"Enter the scenario choice (1-6), or 'Cancel' to exit: \")\n","\n","    if user_choice.lower() == 'cancel':\n","        print(\"Exiting the program.\")\n","        break\n","\n","    elif user_choice == '1':\n","        print(\"Text Classification\")\n","        user_text = input(\"Enter a text to classify: \")\n","        predicted_label = predict_hate_speech(user_text, bert_model, tokenizer, label_encoder, torch_device)\n","        print(f'Text: \"{user_text}\"')\n","        print(f'Predicted Label: {predicted_label}')\n","        print()\n","        print()\n","\n","    elif user_choice == '2':\n","        print(\"Audio Classification\")\n","        # Upload audio file\n","        uploaded = files.upload()\n","        audio_filename = list(uploaded.keys())[0]\n","        audio_content = uploaded[audio_filename]\n","        predicted_label_audio = predict_hate_speech_audio(audio_content, bert_model, tokenizer, label_encoder, torch_device)\n","        print(f'Hate Speech Prediction for Audio: {predicted_label_audio}')\n","        print()\n","        print()\n","\n","    elif user_choice == '3':\n","        print(\"Image Classification\")\n","        # Upload image file\n","        uploaded_files = files.upload()\n","        image_content = list(uploaded_files.values())[0]\n","        predicted_label_image = predict_hate_speech_image(image_content, bert_model, tokenizer, label_encoder, torch_device)\n","        print(f'Hate Speech Prediction for Image: {predicted_label_image}')\n","        print()\n","\n","    elif user_choice == '4':\n","        print(\"GIF Classification\")\n","        # Scenario 4: Video file entered by the user\n","        # Set up authentication for Video Intelligence API\n","        os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = r\"/content/drive/MyDrive/Hate Speech Detection Project/Project Execution/Backend Development/Hate Speech Detection Models/Mark 8/Text Detection/noted-casing-413617-bb0c110a54de.json\"\n","\n","        # Allow the user to upload a video\n","        from google.colab import files\n","        video_upload = files.upload()\n","\n","        # Ensure there is exactly one video file\n","        assert len(video_upload) == 1, \"Please upload one video file at a time.\"\n","\n","        video_file_name = next(iter(video_upload))  # Get filename directly\n","        video_content = video_upload[video_file_name]\n","\n","        # Perform text detection on the uploaded video\n","        client = videointelligence.VideoIntelligenceServiceClient()\n","\n","        # Configure the request\n","        features = [Feature.TEXT_DETECTION]\n","        config = videointelligence.TextDetectionConfig(language_hints=[\"en\"])\n","        context = videointelligence.VideoContext(text_detection_config=config)\n","\n","        request = videointelligence.AnnotateVideoRequest(\n","            input_content=video_content,\n","            features=features,\n","            video_context=context\n","        )\n","\n","        # Execute the request\n","        operation = client.annotate_video(request)\n","        print(\"Started text detection operation. Waiting for results...\")\n","        results = operation.result(timeout=300)\n","\n","        # Get and print results\n","        annotation_results = results.annotation_results[0]\n","        extracted_text = \"\"\n","        for text_annotation in annotation_results.text_annotations:\n","            extracted_text += text_annotation.text + \" \"\n","\n","        # Apply Hate Speech Detection on Extracted Text\n","        predicted_label = predict_hate_speech(extracted_text, bert_model, tokenizer, label_encoder, torch_device)\n","\n","        # Print or use the result\n","        print(f'Text Extracted from GIF: \"{extracted_text}\"')\n","        print(f'Hate Speech Detection Result: {predicted_label}')\n","        print()\n","        print()\n","\n","    elif user_choice == '5':\n","        print(\"Video Classification\")\n","        # Allow the user to upload a video with background audio\n","        video_upload = files.upload()\n","\n","        # Ensure there is exactly one video file\n","        assert len(video_upload) == 1, \"Please upload one video file at a time.\"\n","\n","        # Extract the file content from the dictionary\n","        video_file_name = list(video_upload.keys())[0]\n","        video_content = video_upload[video_file_name]\n","\n","        # Save the video content to a temporary file\n","        with open(video_file_name, 'wb') as video_file:\n","            video_file.write(video_content)\n","\n","        print()\n","        print(\"Task 1 : Audio inside Video is processed\")\n","        # Use the temporary file to create VideoFileClip\n","        clip = mp.VideoFileClip(video_file_name)\n","\n","        # Save the audio file to the specified location\n","        audio_output_path = r\"/content/drive/MyDrive/Hate Speech Detection Project/video_to_udio.mp3\"\n","        clip.audio.write_audiofile(audio_output_path)\n","\n","        # Upload audio file\n","        uploaded = audio_output_path\n","        audio_content = open(uploaded, 'rb').read()\n","        predicted_label_audio = predict_hate_speech_audio(audio_content, bert_model, tokenizer, label_encoder, torch_device)\n","        print(f'Hate Speech Prediction for Audio: {predicted_label_audio}')\n","        print()\n","\n","        print(\"Task 2 : Text inside Video is processed\")\n","        os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = r\"/content/drive/MyDrive/Hate Speech Detection Project/Project Execution/Backend Development/Hate Speech Detection Models/Mark 8/Text Detection/noted-casing-413617-bb0c110a54de.json\"\n","\n","        video_file_name = list(video_upload.keys())[0]\n","        video_content = video_upload[video_file_name]\n","\n","        # Perform text detection on the uploaded video\n","        client = videointelligence.VideoIntelligenceServiceClient()\n","\n","        # Configure the request\n","        features = [Feature.TEXT_DETECTION]\n","        config = videointelligence.TextDetectionConfig(language_hints=[\"en\"])\n","        context = videointelligence.VideoContext(text_detection_config=config)\n","\n","        request = videointelligence.AnnotateVideoRequest(\n","            input_content=video_content,\n","            features=features,\n","            video_context=context\n","        )\n","\n","        # Execute the request\n","        operation = client.annotate_video(request)\n","        print(\"Started text detection operation. Waiting for results...\")\n","        results = operation.result(timeout=300)\n","\n","        # Get and print results\n","        annotation_results = results.annotation_results[0]\n","        extracted_text = \"\"\n","        for text_annotation in annotation_results.text_annotations:\n","            extracted_text += text_annotation.text + \" \"\n","\n","        # Apply Hate Speech Detection on Extracted Text\n","        predicted_label = predict_hate_speech(extracted_text, bert_model, tokenizer, label_encoder, torch_device)\n","\n","        # Print or use the result\n","        print(f'Text Extracted from GIF: \"{extracted_text}\"')\n","        print(f'Hate Speech Detection Result: {predicted_label}')\n","        print()\n","        print()\n","\n","    elif user_choice == \"6\":\n","        print(\"Youtube Live Classification\")\n","        # Scenario 6: Live YouTube Video\n","        live_video_id = input(\"Enter the YouTube live stream video ID: \")\n","        live_comments_df = get_live_comments_with_timeout(live_video_id, timeout_seconds=60)\n","        classified_live_comments = classify_live_comments(live_comments_df, bert_model, tokenizer, label_encoder, torch_device)\n","        print(classified_live_comments)\n","        print()\n","\n","    else:\n","        print(\"Invalid choice. Please enter a valid scenario choice (1-5) or 'Cancel'.\")\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"collapsed_sections":["D9OKI9uuMoAC","7sqG63SKMvCq","ZZHkpH7DM_eg","F5vR8jeqNDLe"]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"6ce40a102bb349958df56f815697a17f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_652d793d5c294f0fa7473850e1893f6b","IPY_MODEL_cf74e34b7d48410fb554066a44ead22f","IPY_MODEL_1c5c56fd03e446d09cd380e329dcfd44"],"layout":"IPY_MODEL_8c9096046534404e8b0f78adc1f7cc57"}},"652d793d5c294f0fa7473850e1893f6b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bc990974955b48879f148f72aad63802","placeholder":"","style":"IPY_MODEL_7397f954506249edb9caa9d5e2025b63","value":"tokenizer_config.json:100%"}},"cf74e34b7d48410fb554066a44ead22f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4d088bd94cbc450d9d0efd548b25db2d","max":48,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9e6be062e8ca47cda7af67a11aa92f3b","value":48}},"1c5c56fd03e446d09cd380e329dcfd44":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_74e14952bb4949ab916561bde189600e","placeholder":"","style":"IPY_MODEL_8b04abcce86949b684061627cf47ac93","value":"48.0/48.0[00:00&lt;00:00,792B/s]"}},"8c9096046534404e8b0f78adc1f7cc57":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bc990974955b48879f148f72aad63802":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7397f954506249edb9caa9d5e2025b63":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4d088bd94cbc450d9d0efd548b25db2d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9e6be062e8ca47cda7af67a11aa92f3b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"74e14952bb4949ab916561bde189600e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8b04abcce86949b684061627cf47ac93":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bdc56fe63878491bb075fb65a0de8aa6":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c07f78d4ee1e416ca71291b1c94a4a89","IPY_MODEL_bef766e410c04507ac6427db397bdbce","IPY_MODEL_4bc04a3f630b4061ac32ddcb2db9e567"],"layout":"IPY_MODEL_243032496ddc40f9aaccf97bfd7d874f"}},"c07f78d4ee1e416ca71291b1c94a4a89":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fab1ac409f9a4175bbc5da65c3e1e1ce","placeholder":"","style":"IPY_MODEL_2bab0914e1b34841bce78f200fdef377","value":"vocab.txt:100%"}},"bef766e410c04507ac6427db397bdbce":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e16cfd431df0464bbfb25c2ce7647e77","max":871891,"min":0,"orientation":"horizontal","style":"IPY_MODEL_827382cacd204da49f7cdd6f4eef31cd","value":871891}},"4bc04a3f630b4061ac32ddcb2db9e567":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4708dc2c5b754326855485c040913963","placeholder":"","style":"IPY_MODEL_abe00225682d4f7680a06a2f25e468e4","value":"872k/872k[00:00&lt;00:00,3.66MB/s]"}},"243032496ddc40f9aaccf97bfd7d874f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fab1ac409f9a4175bbc5da65c3e1e1ce":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2bab0914e1b34841bce78f200fdef377":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e16cfd431df0464bbfb25c2ce7647e77":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"827382cacd204da49f7cdd6f4eef31cd":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4708dc2c5b754326855485c040913963":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"abe00225682d4f7680a06a2f25e468e4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4a45c64ddbe14af6ae62bc9b830fbbc6":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ccd81dc7a9ef445bbac776baad876836","IPY_MODEL_dac9a0c8efac40e4ade2ffc8e9644c53","IPY_MODEL_4c123d3be27f4f95bfe17d1cf4de4f6c"],"layout":"IPY_MODEL_46c2cadd9b324eee8f1688eed416bf54"}},"ccd81dc7a9ef445bbac776baad876836":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_779765c8af3144fabe0b376159cf744a","placeholder":"","style":"IPY_MODEL_9c46bdc272cd4d9da40bbdd827575f9a","value":"tokenizer.json:100%"}},"dac9a0c8efac40e4ade2ffc8e9644c53":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_bc9796f07ed045109208d36c9ae9eaf5","max":1715180,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3b36f7e89c6443fab2a71eccf7ec4e15","value":1715180}},"4c123d3be27f4f95bfe17d1cf4de4f6c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1d31317f901b4bffbf39a138dabae8bc","placeholder":"","style":"IPY_MODEL_9ee886a69e544669ac512b1798ed18fb","value":"1.72M/1.72M[00:00&lt;00:00,5.60MB/s]"}},"46c2cadd9b324eee8f1688eed416bf54":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"779765c8af3144fabe0b376159cf744a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9c46bdc272cd4d9da40bbdd827575f9a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bc9796f07ed045109208d36c9ae9eaf5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3b36f7e89c6443fab2a71eccf7ec4e15":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1d31317f901b4bffbf39a138dabae8bc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9ee886a69e544669ac512b1798ed18fb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"949be1ef70744078807f0d4e7528b5a8":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e3396e3346954e55b1ea363da3a32927","IPY_MODEL_aa6955c1807441df948d323d6a645240","IPY_MODEL_aea5f874c1d3400b96dd4372ec05096f"],"layout":"IPY_MODEL_30bb877dd92543c89eca3a995bc81caf"}},"e3396e3346954e55b1ea363da3a32927":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_abf5bb3491e5492592040c64b88c9f05","placeholder":"","style":"IPY_MODEL_6821d48ba2c34c859831c989ad4748b5","value":"config.json:100%"}},"aa6955c1807441df948d323d6a645240":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1a0a1887b99e4c03b9f219afca28a2a2","max":625,"min":0,"orientation":"horizontal","style":"IPY_MODEL_98726540e6894a2d952bc52d3455ccea","value":625}},"aea5f874c1d3400b96dd4372ec05096f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e09c73b386a34687af1176b295f6ba8c","placeholder":"","style":"IPY_MODEL_01852540562341788750bfe2f25d374a","value":"625/625[00:00&lt;00:00,48.6kB/s]"}},"30bb877dd92543c89eca3a995bc81caf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"abf5bb3491e5492592040c64b88c9f05":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6821d48ba2c34c859831c989ad4748b5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1a0a1887b99e4c03b9f219afca28a2a2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"98726540e6894a2d952bc52d3455ccea":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e09c73b386a34687af1176b295f6ba8c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"01852540562341788750bfe2f25d374a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2da1804921da4dfe878b825a764ad488":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6d71b335a0f646098fcaf4472c470ceb","IPY_MODEL_6fb15e1dad8c457c8ed702f6333c7c3f","IPY_MODEL_87f3059573914594ab7d2432acc6edda"],"layout":"IPY_MODEL_dcc312e075b849b9acd33035fe574b18"}},"6d71b335a0f646098fcaf4472c470ceb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e6bee8f8b9534e9cabce0412bc5b2696","placeholder":"","style":"IPY_MODEL_138a2008312643e8a5f21ffdc50616a1","value":"model.safetensors:100%"}},"6fb15e1dad8c457c8ed702f6333c7c3f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_469e4dcb49194a41bc628c050406e470","max":672247920,"min":0,"orientation":"horizontal","style":"IPY_MODEL_11d03462c5e645b3a80cae6bbe5c0303","value":672247920}},"87f3059573914594ab7d2432acc6edda":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_04ea76eb366e4ba381936713e1a5b63c","placeholder":"","style":"IPY_MODEL_d89b3024aa41400d82a7f49cb65a4c53","value":"672M/672M[00:02&lt;00:00,255MB/s]"}},"dcc312e075b849b9acd33035fe574b18":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e6bee8f8b9534e9cabce0412bc5b2696":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"138a2008312643e8a5f21ffdc50616a1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"469e4dcb49194a41bc628c050406e470":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"11d03462c5e645b3a80cae6bbe5c0303":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"04ea76eb366e4ba381936713e1a5b63c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d89b3024aa41400d82a7f49cb65a4c53":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}