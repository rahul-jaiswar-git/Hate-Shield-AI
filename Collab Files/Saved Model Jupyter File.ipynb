{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2b41c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: joblib in d:\\anaconda\\lib\\site-packages (1.1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at C:\\Users\\Tanmay Nigade\\Downloads\\Hate Speech Hinglish Laguage\\hate_speech_model were not used when initializing TFBertForSequenceClassification: ['dropout_37']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at C:\\Users\\Tanmay Nigade\\Downloads\\Hate Speech Hinglish Laguage\\hate_speech_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "%pip install joblib\n",
    "import joblib\n",
    "\n",
    "from transformers import TFBertForSequenceClassification, BertTokenizer\n",
    "\n",
    "# Load the BERT model and tokenizer\n",
    "model_directory = r'C:\\Users\\Tanmay Nigade\\Downloads\\Hate Speech Hinglish Laguage\\hate_speech_model'\n",
    "loaded_model = TFBertForSequenceClassification.from_pretrained(model_directory)\n",
    "loaded_tokenizer = BertTokenizer.from_pretrained(model_directory)\n",
    "\n",
    "# Load the TensorFlow model\n",
    "tf_model_filename = r'C:\\Users\\Tanmay Nigade\\Downloads\\Hate Speech Hinglish Laguage\\tf_model.h5'\n",
    "loaded_model.load_weights(tf_model_filename)\n",
    "\n",
    "# Load the label encoder\n",
    "label_encoder_filename = r'C:\\Users\\Tanmay Nigade\\Downloads\\Hate Speech Hinglish Laguage\\label_encoder.pkl'\n",
    "loaded_label_encoder = joblib.load(label_encoder_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2a08b1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at C:\\Users\\Tanmay Nigade\\Downloads\\Hate Speech Hinglish Laguage\\hate_speech_model were not used when initializing TFBertForSequenceClassification: ['dropout_37']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at C:\\Users\\Tanmay Nigade\\Downloads\\Hate Speech Hinglish Laguage\\hate_speech_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 11s 11s/step\n",
      "Predicted Label: yes\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "from transformers import TFBertForSequenceClassification, BertTokenizer\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "def load_model_and_predict(text):\n",
    "    # Load the BERT model and tokenizer\n",
    "    model_directory = r'C:\\Users\\Tanmay Nigade\\Downloads\\Hate Speech Hinglish Laguage\\hate_speech_model'\n",
    "    loaded_model = TFBertForSequenceClassification.from_pretrained(model_directory)\n",
    "    loaded_tokenizer = BertTokenizer.from_pretrained(model_directory)\n",
    "\n",
    "    # Load the TensorFlow model\n",
    "    tf_model_filename = r'C:\\Users\\Tanmay Nigade\\Downloads\\Hate Speech Hinglish Laguage\\tf_model.h5'\n",
    "    loaded_model.load_weights(tf_model_filename)\n",
    "\n",
    "    # Load the label encoder\n",
    "    label_encoder_filename = r'C:\\Users\\Tanmay Nigade\\Downloads\\Hate Speech Hinglish Laguage\\label_encoder.pkl'\n",
    "    loaded_label_encoder = joblib.load(label_encoder_filename)\n",
    "\n",
    "    # Tokenize and preprocess the input text\n",
    "    encoding = loaded_tokenizer.encode_plus(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=128,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='tf'\n",
    "    )\n",
    "\n",
    "    input_ids = encoding['input_ids']\n",
    "    attention_mask = encoding['attention_mask']\n",
    "\n",
    "    # Make prediction\n",
    "    with tf.device('/cpu:0'):  # Ensure predictions are made on CPU\n",
    "        outputs = loaded_model.predict([input_ids, attention_mask])\n",
    "        logits = outputs.logits\n",
    "\n",
    "    # Convert logits to probabilities and get the predicted label\n",
    "    probabilities = tf.nn.softmax(logits, axis=1).numpy()[0]\n",
    "    predicted_label_id = np.argmax(probabilities)\n",
    "    predicted_label = loaded_label_encoder.classes_[predicted_label_id]\n",
    "\n",
    "    return predicted_label\n",
    "\n",
    "# Example usage:\n",
    "user_input = input(\"Enter a text to classify: \")\n",
    "predicted_label = load_model_and_predict(user_input)\n",
    "print(f'Predicted Label: {predicted_label}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8787d72d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
